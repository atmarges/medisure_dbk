{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0c26cb-abf3-4e19-aca8-216a5cc38d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "from config.config import *\n",
    "from config.schemas import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4adab1-5271-4e2c-bf50-3bdb3ffa1f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_to_delta(df_source, target_table, merge_condition):\n",
    "    \"\"\"\n",
    "    Upserts data from a Spark DataFrame into a Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    df_source (DataFrame): Source Spark DataFrame to upsert.\n",
    "    target_table (str): Name of the target Delta table.\n",
    "    merge_condition (str): SQL merge condition for matching records.\n",
    "\n",
    "    Behavior:\n",
    "    - If the target Delta table exists, performs a merge (upsert) using the provided condition.\n",
    "      Inserts all records from the source DataFrame that do not match the target.\n",
    "    - If the target table does not exist, creates it and overwrites with the source DataFrame.\n",
    "\n",
    "    Example:\n",
    "        upsert_to_delta(\n",
    "            df_source=df,\n",
    "            target_table=\"my_delta_table\",\n",
    "            merge_condition=\"t.id = s.id\"\n",
    "        )\n",
    "    \"\"\"\n",
    "    if spark.catalog.tableExists(target_table):\n",
    "        delta = DeltaTable.forName(spark, target_table)\n",
    "        (\n",
    "            delta\n",
    "                .alias(\"t\")\n",
    "                .merge(df_source.alias(\"s\"), merge_condition)\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute()\n",
    "        )\n",
    "    else:\n",
    "        df_source.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df10e568-485e-4e03-82ed-5c3415fe916a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1758084744757}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_claims = spark.table(st_fact_claims_merged)\n",
    "df_claims.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63dc6bbd-a51c-441f-b8cb-dfd0d2472d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Clean and transform st_claims_merged table\n",
    "# ------------------------------\n",
    "\n",
    "# Load st_claims_merged table\n",
    "df_claims = spark.table(st_fact_claims_merged)\n",
    "\n",
    "# Remove nulls\n",
    "df_claims = (\n",
    "    df_claims\n",
    "        .filter(F.col(\"ClaimID\").isNotNull())\n",
    "        .filter(F.col(\"MemberID\").isNotNull())\n",
    "        .filter(F.col(\"ProviderID\").isNotNull())\n",
    ")\n",
    "\n",
    "# Cast to correct data types\n",
    "df_claims = (\n",
    "    df_claims\n",
    "        .withColumn(\"ClaimDate\", F.to_date(F.col(\"ClaimDate\"), \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"Amount\", F.col(\"Amount\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "# Convert string list to an array\n",
    "df_claims = (df_claims\n",
    "    .withColumn(\"ICD10Codes\", F.split(F.col(\"ICD10Codes\"), \";\"))\n",
    "    .withColumn(\"CPTCodes\", F.split(F.col(\"CPTCodes\"), \";\"))\n",
    ")\n",
    "\n",
    "# Check data quality\n",
    "df_claims = (df_claims\n",
    "    .withColumn(\"dq_amount_valid\", F.when(F.col(\"Amount\") > 0, True).otherwise(False))\n",
    "    .withColumn(\"dq_has_member\", F.col(\"MemberID\").isNotNull())\n",
    ")\n",
    "\n",
    "# Foreign Key Validation\n",
    "members = spark.table(bt_ref_members).select(\"MemberID\").distinct()\n",
    "providers = spark.table(bt_ref_providers).select(\"ProviderID\").distinct()\n",
    "\n",
    "df_claims = (\n",
    "    df_claims\n",
    "        .join(members, \"MemberID\", \"left\")\n",
    "            .withColumn(\"dq_member_valid\", F.col(\"MemberID\").isNotNull())\n",
    "        .join(providers, \"ProviderID\", \"left\")\n",
    "            .withColumn(\"dq_provider_valid\", F.col(\"ProviderID\").isNotNull())\n",
    ")\n",
    "\n",
    "# Create ID for deduplication\n",
    "df_claims = df_claims.withColumn(\n",
    "    \"dedupe_hash\",\n",
    "    F.sha2(F.concat_ws(\"||\", F.col(\"ClaimID\"), F.col(\"MemberID\"), F.col(\"ProviderID\")), 256)\n",
    ")\n",
    "\n",
    "upsert_to_delta(\n",
    "    df_claims, target_table=st_fact_claims_enriched,\n",
    "    merge_condition=\"t.ClaimID = s.ClaimID AND t.dedupe_hash = s.dedupe_hash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe8eef2-9d83-4472-8b57-2ff01005872e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Clean and transform bt_diagnosis table\n",
    "# ------------------------------\n",
    "\n",
    "# Load bt_ref_diagnosis table\n",
    "df_diagnosis = spark.table(bt_ref_diagnosis)\n",
    "\n",
    "# Remove nulls\n",
    "df_diagnosis = (\n",
    "    df_diagnosis\n",
    "        .filter(F.col(\"Code\").isNotNull())\n",
    "        .filter(F.col(\"Description\").isNotNull())\n",
    ")\n",
    "\n",
    "# Perform upsert to target delta table\n",
    "upsert_to_delta(\n",
    "    df_diagnosis, target_table=st_ref_diagnosis_clean,\n",
    "    merge_condition=\"t.Code = s.Code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899e2879-a745-4b46-9748-a0bfdf0f3fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Clean and transform bt_ref_members table\n",
    "# ------------------------------\n",
    "\n",
    "# Load bt_ref_members table\n",
    "df_members = spark.table(bt_ref_members)\n",
    "\n",
    "# Remove nulls\n",
    "df_members = (\n",
    "    df_members\n",
    "        .filter(F.col(\"MemberID\").isNotNull())\n",
    "        .filter(F.col(\"PlanType\").isNotNull())\n",
    "        .filter(F.col(\"Name\").isNotNull())\n",
    ")\n",
    "\n",
    "# Check if valid email\n",
    "df_members = (\n",
    "    df_members.withColumn(\"dq_email_valid\", \n",
    "        F.col(\"Email\").rlike(r\"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$\"))\n",
    ")\n",
    "\n",
    "# Perform upsert to target delta table\n",
    "upsert_to_delta(\n",
    "    df_members, target_table=st_ref_members_clean,\n",
    "    merge_condition=\"t.MemberID = s.MemberID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4a1b1d-507c-48a5-be21-631fedffe085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Clean and transform bt_ref_providers table\n",
    "# ------------------------------\n",
    "\n",
    "# Load bt_ref_providers table\n",
    "df_providers = spark.table(bt_ref_providers)\n",
    "\n",
    "# Remove nulls\n",
    "df_providers = (\n",
    "    df_providers\n",
    "        .filter(F.col(\"ProviderID\").isNotNull())\n",
    "        .filter(F.col(\"Name\").isNotNull())\n",
    ")\n",
    "\n",
    "# Create separate columns for location\n",
    "df_providers = (\n",
    "    df_providers.select(\n",
    "        \"*\",\n",
    "        F.col(\"Locations\")[\"Address\"].alias(\"Address\"),\n",
    "        F.col(\"Locations\")[\"City\"].alias(\"City\"),\n",
    "        F.col(\"Locations\")[\"State\"].alias(\"State\")\n",
    "    )\n",
    ")\n",
    "df_providers = df_providers.drop(\"Locations\")\n",
    "\n",
    "# Perform upsert to target delta table\n",
    "upsert_to_delta(\n",
    "    df_providers, target_table=st_ref_providers_clean,\n",
    "    merge_condition=\"t.ProviderID = s.ProviderID\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_silver_clean_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
